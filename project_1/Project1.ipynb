{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfUt7BmNzkIx"
   },
   "source": [
    "# Project 1: First steps in Machine Learning (70 Points)\n",
    "In this project, you will train and evaluate your first machine learning models. We provide a structure with a lot of **TODO**s guiding you through the work. Please read the following information carefully.\n",
    "\n",
    "## Grading\n",
    "You can gain a total of 70 points in this project.\n",
    "\n",
    "Please follow the **TODO**s in this notebook. There are practical and theoretical tasks to do.<br>\n",
    "When working on the tasks please consider the following information:\n",
    "* write short texts in **full sentences** answering the **TODO**s. Note, that points are given primarily for reporting and analyzing results (assuming you provide the code to reproduce these results). \n",
    "* have a look at all imports in this notebook; they already define which method you should use\n",
    "\n",
    "\n",
    "## Organizational and Deadline\n",
    "On Monday, **November 11th**, there will be a Q&A session in the tutorials! Start to work on this project **from now on** and take the offer to resolve any remaining ambiguity.\n",
    "This assignment is due on **November 19th**. Please upload your solution to the Moodle as an ipynb-file.<br>\n",
    "For a submission you need to be part of an assignment group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCypy1U8zkI0"
   },
   "source": [
    "# Part 1: Training your first models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) kNN - Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "197KcJbGzkI3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# load dataset_1.npz\n",
    "# the data and labels are saved in X and y, respectively\n",
    "data_set = np.load('public/dataset_1.npz')\n",
    "X = data_set['X']\n",
    "y = data_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first dataset\n",
    "\n",
    "**TODO:** Take a closer look at the dataset, e.g. number of samples, dimensionality, labels, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "For the first part of the exercise, use this data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.7*X.shape[0])\n",
    "n_test_val = int(0.15*X.shape[0])\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:n_train+n_test_val]\n",
    "y_test = y[n_train:n_train+n_test_val]\n",
    "X_val = X[n_train+n_test_val:]\n",
    "y_val = y[n_train+n_test_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Train and evaluate a kNN classifier with k=3 and report the model's accuracy on the train, validation and test set. Use the data as splitted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoR7mM5o81n-"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Report your results. Are there difference in the train/validation/test accuracy? Why? Does the model perform well on the dataset?\n",
    "\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Train and test the kNN classifier for different values of k on dataset 1.\n",
    "- Use the train set to train the classifier and the validation set to evaluate the performance.\n",
    "- Plot the validation accuracy for different values of k. Choose all possible values for k.\n",
    "- Finally train the kNN classifier with the optimal k and report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FY4-FS3LzkJI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoj8y_WWzkJQ"
   },
   "source": [
    "**TODO:** Answer the following questions in a **full text**.\n",
    "\n",
    "* Describe your choice of values of k. Why did you choose them?\n",
    "> ...\n",
    "* For which values of k does the model perform best? How did you determine this?\n",
    "> ...\n",
    "* Would this value perform best on another dataset as well? Why/ Why not?\n",
    "> ...\n",
    "* What is the smallest and the greatest possible value for k? What would happen if we would choose these special values?\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxex3z5tzkJR"
   },
   "source": [
    "## 1b) Logistic Regression\n",
    "Let's try another model as well. We will use the same dataset and split as before.\n",
    "\n",
    "**TODOs**: \n",
    "- Train and evaluate logistic regression on the given split of dataset 1. Report the train, test and validation accuracy.\n",
    "- Plot the dataset and decision boundary (own implementation, see lecture slides 53/54 in slides1.pdf) where the decision boundary is optimized on the complete dataset (X).\n",
    "- Based on the accuracy and plot, answer the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nBBLXJMBSqI"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** How does Logistic Regression perform on this dataset? How does this relate to the properties of the model and the dataset? Answer in full sentences.\n",
    ">..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Preprocessing\n",
    "\n",
    "To achieve better performance, we apply polynomial preprocessing before fitting logistic regression. This can be done by constructing a pipeline as shown below.\n",
    "\n",
    "**TODO:** Optimize the polynomial degree and plot the performance for different values of `degree`. Choose reasonable values of `degree`.  \n",
    "* <font color='red'>Important: </font> Pass only one degree to PolynomialFeatures (not a tuple (min,max)) at each time and validate the whole pipeline! Remember to use the train set for training and the validation set to determine the optimal polynomial degree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = [('poly', PolynomialFeatures(degree=1)), ('clf', LogisticRegression(max_iter=10000))]\n",
    "pipe = Pipeline(estimators)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Given the best polynomial degree, train your model again, report the test accuracy and plot the decision boundary.\n",
    "\n",
    "- Here you can use plot_2d_decisionboundary() from utils.py (see the following import), because plotting the decision boundary with preprocessing is more complicated.\n",
    "Instead of calculating the decision boundary exactly, the function uses a grid-based approach, where each (x,y) position in the plot is colored according to the prediction of the estimator given (x,y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_2d_decisionboundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Report your results. Answer the following questions in full sentences.\n",
    "\n",
    "* What degrees did you try out and why?  \n",
    "> ...\n",
    "\n",
    "* For what degree does the pipeline perform best?  \n",
    "> ...\n",
    "\n",
    "* Would this polynomial degree also perform best on another dataset?  \n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edHXCg-IzkJg"
   },
   "source": [
    "## 1c) Comparing kNN and Logistic Regression\n",
    "We want you to compare the kNN, logistic regression (without preprocessing) and polynomial regression (logistic regression with polynomial preprocessing). Use the previous dataset (dataset_1.npz) and dataset_2_a.npz.\n",
    "\n",
    "**TODO:** Compare the performance of the kNN, Logistic Regression and Polynomial Regression. You can skip the hyperparameter tuning and use the best k and polynomial degree from before. Therefore, a simple train-test split is sufficient (e.g. 70:30)\n",
    "- compare the accuracies of the three models/pipelines\n",
    "- plot the decision boundaries (feel free to use plot_2d_decisionboundary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOIj3_vUzkJh"
   },
   "outputs": [],
   "source": [
    "from utils import plot_classification_dataset # plot the data without decision boundary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle  \n",
    "\n",
    "def train(dataset):\n",
    "    \"\"\"\n",
    "    Train kNN & Log.Reg. on a given dataset and plot the dataset as well as the\n",
    "    model's decision boundary\n",
    "    \n",
    "    Params:\n",
    "        dataset: name of the datase\n",
    "    \n",
    "    Examples:\n",
    "        train('dataset_1')\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    pass # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"public/dataset_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"public/dataset_2_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANBVeSXjzkJn"
   },
   "source": [
    "**TODO:** Describe your results and analyze them: Which model(s) perform(s) best on dataset 1 and 2? Is there a difference in the datasets causing this behavior? Take the classifier properties into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model evaluation\n",
    "\n",
    "Consider the following scenario: Two groups of students work on this assignment. They both use the function below to generate a train-test split and compare the performance of kNN and Logistic Regression. For simplicity you can ignore hyperparameter tuning and use `k=5` and `deg=2`. However both groups achieve different results. The code snippet below shows how they handled the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = np.load('public/dataset_2_b.npz')\n",
    "X = data_set['X']\n",
    "y = data_set['y']\n",
    "\n",
    "n = X.shape[0]\n",
    "\n",
    "def group1_eval():\n",
    "    print(\"group1:\")\n",
    "    n_train = int(4*n/5)\n",
    "    X_train = X[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_train = y[:n_train]\n",
    "    y_test = y[n_train:]\n",
    "    \n",
    "    # TODO: train and evaluate a kNN classifier with k=5 and report the accuracy\n",
    "\n",
    "    \n",
    "    # TODO:train and evaluate a logistic regression pipeline with polynomial preprocessing\n",
    "\n",
    "    \n",
    "    \n",
    "def group2_eval():\n",
    "    print(\"group2\")\n",
    "    n_test = int(n/5)\n",
    "    X_train = X[n_test:]\n",
    "    X_test = X[:n_test]\n",
    "    y_train = y[n_test:]\n",
    "    y_test = y[:n_test]\n",
    "    \n",
    "    # TODO:train and evaluate a kNN classifier with k=5 and report the accuracy\n",
    "\n",
    "    \n",
    "    # TODO train and evaluate a logistic regression pipeline with polynomial preprocessing\n",
    "    \n",
    "    \n",
    "group1_eval()\n",
    "group2_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Explain why they achieve different results. Can you think of any better strategy than the simple train-test split to make the results more comparable? Feel free to add some code to underline your points or show your recommended approach.\n",
    "\n",
    "* What do you think, whose results are correct?\n",
    "> ...\n",
    "* How could they improve their evaluation to be more confident about the results? (hint: If we make a statement like classifier 1 outperforms classifier 2 on this dataset, the statement should be representative for the whole dataset. As always, efficient solutions are preferable!)\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Implement your recommended approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Yet another Dataset\n",
    "\n",
    "**TODO:** Load dataset 3. Train and report the accuracy for kNN and logistic regression (without polynomial preprocessing).\n",
    "You may use a simple train-test split or the strategy you proposed in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = np.load('public/dataset_3.npz')\n",
    "X = data_set['X']\n",
    "y = data_set['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Report the accuracy for each class. This can be done, by computing the accuracy score only on samples that belong to class i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Describe your results and analyze them: Do you observe any problematic behavior?\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Analyze the dataset. Focus on possible reasons for the above mentioned problems. Consider using plots to visualize the dataset's properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Discuss the properties of the dataset and how they influence the performance of the classifiers.\n",
    "\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Given your results and the properties of the data, reflect on accuracy (over all classes) as a metric. Propose another metric that would be better suited for this kind of dataset and explain why. Adjustments to accuracy are fine, too.\n",
    "\n",
    "\n",
    "> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HCypy1U8zkI0",
    "qxex3z5tzkJR",
    "edHXCg-IzkJg"
   ],
   "name": "Copy of Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
